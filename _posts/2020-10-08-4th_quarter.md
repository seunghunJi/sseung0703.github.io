---
layout: post
title: "2020_4th_quarter"
date: 2020-10-08
image_url: https://user-images.githubusercontent.com/32321592/98316905-a7b9a000-201e-11eb-8135-053b5662ff85.png
mathjax: true
comments: true
---

# Self-training with Noisy Student improves ImageNet classification
## Qizhe Xie, Minh-Thang Luong, Eduard Hovy, Quoc V. Le 
### 발표자: 강진구 [[paper link](https://arxiv.org/pdf/1911.04252.pdf), [presentation material](https://drive.google.com/file/d/1GIAu3CTYjv5Noj6pLGh7em-66MGW_nfW/view)]
- Semi-supervised 방법인 Noisy student training을 적용하여 ImageNet SOTA를 경신한 논문입니다. 
- 수많은 Unlabeled 데이터들을 잘 활용하는 방법을 소개하는데 그 방법이 매우 간단합니다. Teacher model과 Student model을 정의하고 Teacher model이 Unlabeled 데이터에 대해 labeling해서 Student model에게 주면 Student model은 그 이미지와 Teacher model이 labeling해 준 pseudo-label을 해당 이미지의 label로 여기고 학습합니다. 첫 Teacher model은 사람이 labeling한 데이터셋을 사용합니다. 첫 Teacher model이 만들어낸 Student model은 그 다음 generation의 Teacher model이 되어 새로운 Student model을 만들어 냅니다. 이 방법으로 여러번 Generation을 거치면서 점점 더 좋은 성능의 모델이 만들어 집니다.
- Teacher model이 Unlabeled data를 labeling 할 때는 이미지에 noise를 섞지 않고, Student model이 pseudo-label을 학습할 때는 noise를 강하게 섞는 것이 이 기법의 핵심입니다. 또한 generation이 거듭되면서 Student model은 기존 Teacher model보다 더 많은 파라미터를 가진 큰 모델로 정의함으로써 더 어려운 문제를 풀 수 있게 만들어 줍니다. 
<p align="center">
<img width="538" alt="Screen Shot 2021-01-03 at 12 24 09 AM" src="https://user-images.githubusercontent.com/25892000/103460520-16368800-4d5a-11eb-8400-4fe5048f2c7e.png">

</p>

***
