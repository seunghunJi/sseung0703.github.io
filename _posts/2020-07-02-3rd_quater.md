---
layout: post
title: "2020_3rd_quater"
date: 2020-07-02
image_url: https://user-images.githubusercontent.com/26036843/90788498-d63dae00-e340-11ea-87b5-7a0f37c1d93a.png
mathjax: true
comments: true
---

# Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks
## Zhonghui You, Kun Yan, Jinmian Ye, Meng Ma, Ping Wang, NeurIPS 2019
### 발표자: 이승현 [[paper link](https://arxiv.org/abs/1909.08174), [presentation material](https://trello-attachments.s3.amazonaws.com/5d15b7297b29f54b88064f86/5f3a18b1378bf75d14551518/d9d8f8016b1e58e1877fc6a43b1cd812/GDP.pdf)]
- Filter pruning 기법 중 하나로 기존 기법에 비해 효과적으로 filter의 importance를 정의하고, 이를 기반으로 filter를 제거 및 pruned network를 tuning할 수 있는 방법을 제안한 논문입니다.
- Gate decorator란 Taylor expansion에 기반한 기법으로, pruning할 각 layer에 gate vector를 추가 및 이의 gradient에 기반한 score를 정하며 이는 아래와 같습니다.

$$\Theta(\phi_i) = \sum_{(X, Y) \in \mathcal D}\left| \frac{\delta \mathcal{L}(X,Y;\theta)}{\delta \phi_i} \phi_i \right| $$

- Tick step에서는 gate를 tuning 및 일정 비율을 pruning하며, Tock step에서는 손실된 성능을 복원하기 위해 일정 epoch만큼 fine-tuning 합니다. 이 때 Tock step에서는 gate에 $$L_1$$-regularization을 가해 sparce해지도록 만듭니다.
- Pruning 시에 network의 특성에 따라 서로 종속적인 layer들을 group으로 묶어 하나의 gate를 통해 importance score를 계산 및 동시에 pruning되도록 합니다.
- 높은 성능을 보이며 몇 가지 중요한 insight를 주는 좋은 기법으로 보이지만, pruning 시간이 매우 길다는 것은 큰 단점으로 보입니다.

<p align="center">
  <img src="https://user-images.githubusercontent.com/26036843/90788752-14d36880-e341-11ea-829e-2485ce943e91.png">
  <img src="https://user-images.githubusercontent.com/26036843/90788498-d63dae00-e340-11ea-87b5-7a0f37c1d93a.png">
</p>
